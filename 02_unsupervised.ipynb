{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPfiXBa0J3LDPNogfz5u/by",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wakristensen/machine-learning-workshop/blob/main/02_unsupervised.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 02 Kundefeatures\n",
        "\n",
        "<div class=\"badges\">\n",
        "  <a>‚è±Ô∏è 40 min</a>\n",
        "  <a>üë©‚Äçüíª Hands-on</a>\n",
        "  <a>üì¶ UK Online Retail 2010‚Äì2011</a>\n",
        "</div>\n",
        "\n",
        "---\n",
        "\n",
        "N√• har vi forst√•tt og renset dataene.  \n",
        "\n",
        "Vi har n√• et transaksjonsdatasett som er renset for:\n",
        "- Manglende verdier (CustomerID og Description)  \n",
        "- Duplikater  \n",
        "- Nullpriser (UnitPrice = 0)  \n",
        "- Tjeneste-relaterte beskrivelser (eks. \"Next Day Carriage\")  \n",
        "- Feil / D√•rlige / anomalie *StockCodes* (eks. \"POST\", \"BANK CHARGES\")  \n",
        "---\n",
        "\n",
        "Datasettet best√•r n√• av rene transaksjoner som representerer faktiske kj√∏p.\n",
        "\n",
        "I denne modulen skal vi:\n",
        "- Bygge kundevariabler (features)\n",
        "- Skalere dimensjoner\n",
        "- Redusere antall dimensjoner (PCA)\n",
        "- Kj√∏re K-means clustering\n",
        "- Tolke segmentene"
      ],
      "metadata": {
        "id": "oAA7BH32teuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import av pakker"
      ],
      "metadata": {
        "id": "XziouCH-Nr1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score\n",
        "from yellowbrick.cluster import KElbowVisualizer"
      ],
      "metadata": {
        "id": "V1jODJFYNvIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ferdig kode for √• kj√∏re raskt gjennom rensestegene i forrige notebook\n",
        "file_id = \"1Ku-y5BE5CdGQwzEMQ491cfIYWILS6T8U\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "\n",
        "# Dette matcher stegene vi gjorde tidligere, slik at radantallet skal bli likt.\n",
        "def clean_transactions(df: pd.DataFrame):\n",
        "    out = df.copy()\n",
        "    # 3.1 Fjerne rader uten CustomerID eller Description\n",
        "    out = out.dropna(subset=[\"CustomerID\", \"Description\"])\n",
        "    # 3.2 Fjern helt identiske duplikater\n",
        "    out = out.drop_duplicates()\n",
        "    # 3.3 Merk kansellerte transaksjoner (beholdes, men markeres)\n",
        "    out[\"InvoiceNo\"] = out[\"InvoiceNo\"].astype(str)\n",
        "    out[\"Transaction_Status\"] = np.where(\n",
        "        out[\"InvoiceNo\"].str.startswith(\"C\"), \"Cancelled\", \"Completed\"\n",
        "    )\n",
        "    # 3.6 Fjern rader med UnitPrice <= 0\n",
        "    out = out[out[\"UnitPrice\"] > 0]\n",
        "    # 3.4 Fjern anomale StockCodes (0 eller 1 siffer, f.eks. POST, BANK CHARGES, ...)\n",
        "    unique_codes = out[\"StockCode\"].astype(str).unique()\n",
        "    anomalous_codes = {\n",
        "        code for code in unique_codes if sum(ch.isdigit() for ch in code) in (0, 1)\n",
        "    }\n",
        "    if anomalous_codes:\n",
        "        out = out[~out[\"StockCode\"].astype(str).isin(anomalous_codes)]\n",
        "\n",
        "    # 3.5 Fjern tjeneste-/metadata-beskrivelser og standardiser casing\n",
        "    service_related = {\"NEXT DAY CARRIAGE\", \"HIGH RESOLUTION IMAGE\"}\n",
        "    out = out[~out[\"Description\"].astype(str).str.upper().isin(service_related)]\n",
        "    out[\"Description\"] = out[\"Description\"].astype(str).str.upper().str.strip()\n",
        "\n",
        "    # Reset index til slutt\n",
        "    out = out.reset_index(drop=True)\n",
        "    return out, anomalous_codes\n",
        "\n",
        "# --- Kj√∏r rens ---\n",
        "df = pd.read_csv(url, encoding=\"latin1\")\n",
        "df, anomalous_codes = clean_transactions(df)\n",
        "\n",
        "# --- Rapportering ---\n",
        "EXPECTED_ROWS = 399573  # m√•l: matche radantallet fra del 1\n",
        "print(f\"Rader etter rensing: {len(df)}\")\n",
        "print(f\"Forventet antall rader fra del 1: {EXPECTED_ROWS}\")\n",
        "if len(df) == EXPECTED_ROWS:\n",
        "    print(\"‚úÖ Match med del 1\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Avvik fra del 1. Sjekk at URL og kildedata er identiske, og at cellene i del 1 ble kj√∏rt i samme rekkef√∏lge.\")\n",
        "\n",
        "# Se p√• de f√∏rste radene\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "dbogqyddwThC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fra transaksjoner til kundeprofiler\n",
        "\n",
        "Datasettet v√•rt er transaksjonsbasert (hver rad = en vare i en ordre).  \n",
        "For √• kunne klustre kunder, trenger vi √• lage **kundeprofiler**:\n",
        "\n",
        "- **RFM-variabler:** Recency, Frequency, Monetary  \n",
        "- **Produktdiversitet:** Hvor mange ulike produkter har kunden kj√∏pt?  \n",
        "- **Atferdsvariabler:** N√•r p√• d√∏gnet eller hvilke dager handler kunden?  \n",
        "- **Geografi:** Hovedland for kunden (UK eller ikke)  \n",
        "- **Kanselleringer:** Hvor ofte kansellerer kunden?  \n",
        "\n",
        "Disse nye variablene (features) gir oss et kunde-sentrisk datasett som er egnet for clustering.\n"
      ],
      "metadata": {
        "id": "MQNeShlZynUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se for deg at du er markedsdirekt√∏r.  \n",
        "Hvilke typer kundesegmenter ville du v√¶rt interessert i √• identifisere?  \n",
        "\n",
        "Eksempler:  \n",
        "- H√∏yt verdifulle kunder (stor omsetning, lojale)  \n",
        "- \"Sovende\" kunder (lenge siden sist kj√∏p)  \n",
        "- Sm√•, hyppige kj√∏pere  \n",
        "- Kunder med h√∏y kanselleringsrate  \n",
        "\n",
        "Hvilke segmenter dere tror vil dukke opp n√•r vi bygger features?\n"
      ],
      "metadata": {
        "id": "jzhIxq6NzQhf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 RFM\n",
        "\n",
        "- **Recency**, dager siden siste kj√∏p\n",
        "- **Frequency**, antall transaksjoner\n",
        "- **Monetary**, total spend og snitt pr transaksjon\n",
        "\n",
        "Vi bygger alt i en ny DataFrame som vi kaller `customer_data`.\n"
      ],
      "metadata": {
        "id": "M3WMS94n0zTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Bygg Recency\n",
        "# Finner siste kj√∏psdato per kunde\n",
        "# Finner mest nylige dato i hele datasettet\n",
        "# Regner ut Days_Since_Last_Purchase\n",
        "\n",
        "most_recent = pd.to_datetime(df[\"InvoiceDate\"]).max()\n",
        "\n",
        "customer_data = (\n",
        "    df.groupby(\"CustomerID\")[\"InvoiceDate\"]\n",
        "      .max()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"InvoiceDate\":\"LastPurchaseDate\"})\n",
        ")\n",
        "\n",
        "customer_data[\"LastPurchaseDate\"] = pd.to_datetime(customer_data[\"LastPurchaseDate\"])\n",
        "customer_data[\"Days_Since_Last_Purchase\"] = (most_recent - customer_data[\"LastPurchaseDate\"]).dt.days\n",
        "customer_data = customer_data.drop(columns=[\"LastPurchaseDate\"])\n",
        "\n",
        "customer_data"
      ],
      "metadata": {
        "id": "JoHYYDhE0yS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Frequency\n",
        "total_tx = (\n",
        "    df.groupby(\"CustomerID\")[\"InvoiceNo\"]\n",
        "      .nunique()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"InvoiceNo\":\"Total_Transactions\"})\n",
        ")\n",
        "\n",
        "# Monetary\n",
        "df[\"Total_Spend\"] = df[\"UnitPrice\"] * df[\"Quantity\"]\n",
        "total_spend = (\n",
        "    df.groupby(\"CustomerID\")[\"Total_Spend\"]\n",
        "      .sum()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Merge\n",
        "customer_data = customer_data.merge(total_tx, on=\"CustomerID\", how=\"left\")\n",
        "customer_data = customer_data.merge(total_spend, on=\"CustomerID\", how=\"left\")\n",
        "\n",
        "# Average transaction value\n",
        "customer_data[\"Average_Transaction_Value\"] = (\n",
        "    customer_data[\"Total_Spend\"] / customer_data[\"Total_Transactions\"].replace({0:np.nan})\n",
        ").fillna(0)\n",
        "\n",
        "customer_data"
      ],
      "metadata": {
        "id": "mACLnMA00wtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Total antall produkter (sum av Quantity) og diversitet (antall ulike StockCode)\n",
        "total_qty = (\n",
        "    df.groupby(\"CustomerID\")[\"Quantity\"]\n",
        "      .sum()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"Quantity\":\"Total_Products_Purchased\"})\n",
        ")\n",
        "\n",
        "unique_skus = (\n",
        "    df.groupby(\"CustomerID\")[\"StockCode\"]\n",
        "      .nunique()\n",
        "      .reset_index()\n",
        "      .rename(columns={\"StockCode\":\"Unique_Products_Purchased\"})\n",
        ")\n",
        "\n",
        "customer_data = customer_data.merge(total_qty, on=\"CustomerID\", how=\"left\")\n",
        "customer_data = customer_data.merge(unique_skus, on=\"CustomerID\", how=\"left\")\n",
        "\n",
        "customer_data.head()\n"
      ],
      "metadata": {
        "id": "tpTh2Bsau_jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Hvilke tre kolonner vil du bruke for √• identifisere ‚ÄúHigh value‚Äù kunder, f√∏r skalering og eventuell PCA?"
      ],
      "metadata": {
        "id": "VXWAAtqU1EZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Atferd og geografi\n",
        "\n",
        "Vi legger til:\n",
        "- Average_Days_Between_Purchases\n",
        "- Favorite day og hour\n",
        "- Is_UK basert p√• hovedland\n",
        "- Kanselleringsfrekvens hvis `Transaction_Status` finnes"
      ],
      "metadata": {
        "id": "4qywQFpr1Ibl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Konverterer InvoiceDate til datetime\n",
        "df[\"InvoiceDate\"] = pd.to_datetime(df[\"InvoiceDate\"])\n",
        "\n",
        "# Dag og time\n",
        "df[\"Day_Of_Week\"] = df[\"InvoiceDate\"].dt.dayofweek\n",
        "df[\"Hour\"]        = df[\"InvoiceDate\"].dt.hour\n",
        "\n",
        "# Snitt dager mellom kj√∏p\n",
        "# Bruker InvoiceDay, differanse i dager per kunde\n",
        "days_between = (\n",
        "    df.sort_values([\"CustomerID\",\"InvoiceDate\"]) # Changed from InvoiceDay to InvoiceDate\n",
        "      .groupby(\"CustomerID\")[\"InvoiceDate\"] # Changed from InvoiceDay to InvoiceDate\n",
        "      .apply(lambda s: pd.Series(s).diff().dt.days.dropna().mean() if len(s)>1 else 0)\n",
        "      .reset_index(name=\"Average_Days_Between_Purchases\")\n",
        ")\n",
        "\n",
        "# Favorittdag og favoritt-time\n",
        "fav_day = (\n",
        "    df.groupby([\"CustomerID\",\"Day_Of_Week\"]).size()\n",
        "      .reset_index(name=\"cnt\")\n",
        "      .sort_values([\"CustomerID\",\"cnt\"], ascending=[True,False])\n",
        "      .drop_duplicates(\"CustomerID\")[[\"CustomerID\",\"Day_Of_Week\"]]\n",
        ")\n",
        "\n",
        "fav_hour = (\n",
        "    df.groupby([\"CustomerID\",\"Hour\"]).size()\n",
        "      .reset_index(name=\"cnt\")\n",
        "      .sort_values([\"CustomerID\",\"cnt\"], ascending=[True,False])\n",
        "      .drop_duplicates(\"CustomerID\")[[\"CustomerID\",\"Hour\"]]\n",
        ")\n",
        "\n",
        "customer_data = customer_data.merge(days_between, on=\"CustomerID\", how=\"left\")\n",
        "customer_data = customer_data.merge(fav_day,     on=\"CustomerID\", how=\"left\")\n",
        "customer_data = customer_data.merge(fav_hour,    on=\"CustomerID\", how=\"left\")\n",
        "customer_data"
      ],
      "metadata": {
        "id": "2bGGuC5YvBez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hovedland per kunde og Is_UK\n",
        "cust_country = (\n",
        "    df.groupby([\"CustomerID\",\"Country\"]).size()\n",
        "      .reset_index(name=\"tx\")\n",
        "      .sort_values([\"CustomerID\",\"tx\"], ascending=[True,False])\n",
        "      .drop_duplicates(\"CustomerID\")[[\"CustomerID\",\"Country\"]]\n",
        ")\n",
        "\n",
        "cust_country[\"Is_UK\"] = (cust_country[\"Country\"] == \"United Kingdom\").astype(int)\n",
        "customer_data = customer_data.merge(cust_country[[\"CustomerID\",\"Is_UK\"]], on=\"CustomerID\", how=\"left\")\n",
        "\n",
        "customer_data[\"Is_UK\"].value_counts(dropna=False)\n"
      ],
      "metadata": {
        "id": "QWvqUF4J1Mfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sjekk fornuften i `Average_Days_Between_Purchases`. Hvilke kunder har 0, og er det greit?"
      ],
      "metadata": {
        "id": "0Qk7fB7m1P90"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 | Kanselleringer\n",
        "\n",
        "Vi tar ogs√• med kanselleringsm√∏nstre som features:\n",
        "\n",
        "- **Kanselleringsfrekvens:** Totalt antall kansellerte ordre per kunde.  \n",
        "- **Kanselleringsrate:** Andel kansellerte ordre i forhold til alle ordre per kunde.\n",
        "\n",
        "Variablene hjelper oss √• skille mellom kundetyper (lojale kunder vs. kunder som ofte avbryter kj√∏p).  \n",
        "Vi tar de med som en del av feature-settet.\n"
      ],
      "metadata": {
        "id": "ZEO6tC8gDrK5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Totalt antall ordre per kunde\n",
        "total_transactions = df.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "\n",
        "# Antall kansellerte ordre per kunde\n",
        "cancelled = df[df['Transaction_Status'] == 'Cancelled']\n",
        "cancellation_frequency = cancelled.groupby('CustomerID')['InvoiceNo'].nunique().reset_index()\n",
        "cancellation_frequency.rename(columns={'InvoiceNo': 'Cancellation_Frequency'}, inplace=True)\n",
        "\n",
        "# Sl√• sammen med kunde-data\n",
        "customer_data = pd.merge(customer_data, cancellation_frequency, on='CustomerID', how='left')\n",
        "customer_data['Cancellation_Frequency'].fillna(0, inplace=True)\n",
        "\n",
        "# Regn ut kanselleringsrate\n",
        "customer_data['Cancellation_Rate'] = customer_data['Cancellation_Frequency'] / total_transactions['InvoiceNo']\n",
        "\n",
        "# Se dataene\n",
        "customer_data"
      ],
      "metadata": {
        "id": "CgJQfpu7EiUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Rydding i datatyper\n",
        "customer_data[\"CustomerID\"] = customer_data[\"CustomerID\"].astype(\"string\")\n",
        "customer_data = customer_data.convert_dtypes()\n",
        "\n",
        "print(customer_data.shape)\n",
        "customer_data.head(3)\n",
        "customer_data.info()\n"
      ],
      "metadata": {
        "id": "v9yBhjrW1R1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steg 3. Korrelasjonsanalyse\n",
        "\n",
        "F√∏r vi gj√∏r clustering er det lurt √• inspisere korrelasjon mellom variablene i datasettet.\n",
        "\n",
        "Hvorfor?  \n",
        "- Hvis flere variabler er sterkt korrelert, bidrar de egentlig ikke med unik informasjon.  \n",
        "- Dette kan f√∏re til at clusteringen blir mindre tydelig og mindre meningsfull.\n",
        "\n",
        "Ser vi h√∏y korrelasjon kan vi bruke dimensjonsreduksjonsteknikker som **PCA (Principal Component Analysis)**.  \n",
        "\n",
        "PCA transformerer de korrelerte variablene til et nytt sett av ukorrelerte variabler, samtidig som mest mulig av variasjonen i dataene beholdes.\n",
        "\n",
        "Resultatet blir ofte bedre clustere."
      ],
      "metadata": {
        "id": "gf406srEMdTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Bakgrunnsstil til plotting\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "# Beregn korrelasjonsmatrisen, ekskluder CustomerID (Er ikke feature)\n",
        "corr = customer_data.drop(columns=['CustomerID']).corr()\n",
        "\n",
        "# Fargeskala\n",
        "colors = ['#ff6200', '#ffcaa8', 'white', '#ffcaa8', '#ff6200']\n",
        "my_cmap = LinearSegmentedColormap.from_list('custom_map', colors, N=256)\n",
        "\n",
        "# Lag en maske for √• bare vise nedre trekant av matrisen\n",
        "# (siden korrelasjonsmatrisen er symmetrisk rundt diagonalen)\n",
        "mask = np.zeros_like(corr)\n",
        "mask[np.triu_indices_from(mask, k=1)] = True\n",
        "\n",
        "# Plott korrelasjons-heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(corr, mask=mask, cmap=my_cmap, annot=True, center=0, fmt='.2f', linewidths=2)\n",
        "plt.title('Korrelasjonsmatrise', fontsize=14)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8tTaV0iWNZRV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steg 5 | Elbow-metoden for √• finne antall clustere\n",
        "\n",
        "Elbow-metoden er en teknikk for √• velge optimalt antall clustre (K).  \n",
        "- Vi kj√∏rer KMeans for ulike K-verdier (f.eks. 2 til 15).  \n",
        "- For hvert K beregnes *inertia* (summen av avstander til n√¶rmeste senter).  \n",
        "- Vi plottet inertia mot K.  \n",
        " Punktet kurven begynner √• \"flate ut\" kalles *elbow*, og representerer et godt valg for K (Antall clustre).  \n"
      ],
      "metadata": {
        "id": "Db2rfrKg7dnJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1) Velg datasett for clustering\n",
        "customer_data_cluster = customer_data_scaled.copy()\n",
        "\n",
        "# 2) Dropper kolonner som ikke skal inn i modell\n",
        "drop_cols = ['CustomerID']\n",
        "X = customer_data_cluster.drop(columns=drop_cols, errors='ignore').values\n",
        "\n",
        "# 3) Finn K med enkel Elbow (uten ekstra biblioteker)\n",
        "sns.set(style='darkgrid', rc={'axes.facecolor': '#fcf0dc'})\n",
        "ks = range(1, 11)\n",
        "inertias = []\n",
        "for k in ks:\n",
        "    km = KMeans(n_clusters=k, init='k-means++', n_init=10, max_iter=400, random_state=0)\n",
        "    km.fit(X)\n",
        "    inertias.append(km.inertia_)\n",
        "\n",
        "plt.figure(figsize=(10,4))\n",
        "plt.plot(ks, inertias, marker='o')\n",
        "plt.title('Elbow for KMeans (inertia vs K)')\n",
        "plt.xlabel('K')\n",
        "plt.ylabel('Inertia (WCSS)')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CKPzhd6xHWoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Velg antall clustere for baseline\n",
        "K_BASE = 3\n",
        "km = KMeans(n_clusters=K_BASE, init='k-means++', n_init=10, max_iter=400, random_state=0)\n",
        "labels = km.fit_predict(X)\n",
        "\n",
        "# 5) Legg klyngeetiketter i dataframen\n",
        "customer_data_cluster['cluster'] = labels\n",
        "\n",
        "# 6) Rask sjekk\n",
        "sizes = pd.Series(labels).value_counts().sort_index()\n",
        "sil = silhouette_score(X, labels) if X.shape[0] > K_BASE else np.nan\n",
        "\n",
        "print(f\"Klyngest√∏rrelser: {sizes.to_dict()}\")\n",
        "print(f\"Silhouette-score (baseline): {sil:.3f}\")\n"
      ],
      "metadata": {
        "id": "dNInF4k-zGaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "pca3 = PCA(n_components=3, random_state=0)\n",
        "X_3d = pca3.fit_transform(X)\n",
        "\n",
        "fig = go.Figure()\n",
        "for cl in np.unique(labels):\n",
        "    mask = labels == cl\n",
        "    fig.add_trace(go.Scatter3d(\n",
        "        x=X_3d[mask,0], y=X_3d[mask,1], z=X_3d[mask,2],\n",
        "        mode='markers',\n",
        "        marker=dict(size=4, opacity=0.5),\n",
        "        name=f'Cluster {cl}'\n",
        "    ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title=dict(text=f'3D-visualisering av KMeans K={K_BASE} (PCA, kun for visning)', x=0.5),\n",
        "    scene=dict(\n",
        "        xaxis_title='PC1', yaxis_title='PC2', zaxis_title='PC3',\n",
        "        xaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white'),\n",
        "        yaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white'),\n",
        "        zaxis=dict(backgroundcolor=\"#fcf0dc\", gridcolor='white'),\n",
        "    ),\n",
        "    width=900, height=650\n",
        ")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "aMlElrsL0eE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "customer_data_cluster"
      ],
      "metadata": {
        "id": "gNObTPtF2HdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prosent av kunder i hvert cluster\n",
        "cluster_percentage = (customer_data_cluster['cluster'].value_counts(normalize=True) * 100).reset_index()\n",
        "cluster_percentage.columns = ['Cluster', 'Percentage']\n",
        "cluster_percentage.sort_values(by='Cluster', inplace=True)\n",
        "\n",
        "# Bar plot\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.barplot(x='Percentage', y='Cluster', data=cluster_percentage, orient='h', palette=colors)\n",
        "\n",
        "# Legger prosent p√• bars\n",
        "for index, value in enumerate(cluster_percentage['Percentage']):\n",
        "    plt.text(value+0.5, index, f'{value:.2f}%')\n",
        "\n",
        "plt.title('Kunder per cluster', fontsize=14)\n",
        "plt.xticks(ticks=np.arange(0, 50, 5))\n",
        "plt.xlabel('(%)')\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kSwCls640vLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Steg 10.3 | Evalueringsmetrikker\n",
        "\n",
        "For √• vurdere kvaliteten p√• clusteringen finnes ulike metrikker:\n",
        "\n",
        "Her er tre eksempler p√• standardmetoder\n",
        "\n",
        "- **Silhouette Score**, m√•ler hvor godt punkter passer i egen klynge kontra n√¶rmeste naboklynge. H√∏yere er bedre, omr√•de -1 til 1.  \n",
        "- **Calinski‚ÄìHarabasz Score**, forholdet mellom variasjon *mellom* klynger og *innenfor* klynger. H√∏yere er bedre.  \n",
        "- **Davies‚ÄìBouldin Score**, gjennomsnittlig likhet mellom hver klynge og den mest like klyngen. Lavere er bedre.\n",
        "\n"
      ],
      "metadata": {
        "id": "XvZeGKbG3ga8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BHEa5y8J7yIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
        "\n",
        "# Datasett med 'cluster'-kolonne\n",
        "df_eval = customer_data_cluster.copy()\n",
        "\n",
        "# Trekk ut features (dropp identifikatorer og clusterlabel)\n",
        "drop_cols = [c for c in ['CustomerID', 'cluster'] if c in df_eval.columns]\n",
        "X_eval = df_eval.drop(columns=drop_cols)\n",
        "y_clusters = df_eval['cluster']\n",
        "\n",
        "# Beregn metrikker\n",
        "num_observations = len(df_eval)\n",
        "sil_score = silhouette_score(X_eval, y_clusters, metric='euclidean')\n",
        "calinski_score = calinski_harabasz_score(X_eval, y_clusters)\n",
        "davies_score = davies_bouldin_score(X_eval, y_clusters)\n",
        "\n",
        "# Vis som tabell\n",
        "metrics_df = pd.DataFrame(\n",
        "    {\n",
        "        \"Metric\": [\n",
        "            \"Silhouette Score\",\n",
        "            \"Calinski‚ÄìHarabasz Score\",\n",
        "            \"Davies‚ÄìBouldin Score\",\n",
        "        ],\n",
        "        \"Value\": [\n",
        "            round(sil_score, 6),\n",
        "            round(calinski_score, 6),\n",
        "            round(davies_score, 6),\n",
        "        ],\n",
        "    }\n",
        ")\n",
        "display(metrics_df)\n"
      ],
      "metadata": {
        "id": "ffTqjHU73te9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1) Bruk datasettet direkte (uten StandardScaler) ---\n",
        "df_viz = customer_data_cluster.copy()\n",
        "\n",
        "drop_cols = [c for c in ['CustomerID', 'cluster'] if c in df_viz.columns]\n",
        "feature_cols = [c for c in df_viz.columns if c not in drop_cols]\n",
        "\n",
        "# Beregn gjennomsnitt (centroid) for hver klynge\n",
        "centroids_raw = df_viz.groupby('cluster')[feature_cols].mean()\n",
        "\n",
        "# --- 2) Radaroppsett ---\n",
        "def radar_axes(n_vars):\n",
        "    angles = np.linspace(0, 2*np.pi, n_vars, endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "    return angles\n",
        "\n",
        "def plot_cluster_radar(ax, angles, values, color, title):\n",
        "    vals = values.tolist() + values.tolist()[:1]\n",
        "    ax.fill(angles, vals, alpha=0.25, color=color)\n",
        "    ax.plot(angles, vals, linewidth=2, color=color)\n",
        "    ax.set_title(title, y=1.10, fontsize=12)\n",
        "\n",
        "clusters = centroids_raw.index.tolist()\n",
        "n_clusters = len(clusters)\n",
        "n_vars = len(feature_cols)\n",
        "angles = radar_axes(n_vars)\n",
        "\n",
        "prop_cycle = plt.rcParams['axes.prop_cycle'].by_key().get('color', ['C0','C1','C2','C3','C4','C5'])\n",
        "\n",
        "fig, axes = plt.subplots(1, n_clusters, figsize=(min(6*n_clusters, 20), 8), subplot_kw=dict(polar=True))\n",
        "if n_clusters == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, cl in enumerate(clusters):\n",
        "    ax = axes[i]\n",
        "    color = prop_cycle[i % len(prop_cycle)]\n",
        "    plot_cluster_radar(ax, angles, centroids_raw.loc[cl], color, f\"Cluster {cl}\")\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(feature_cols, fontsize=8, rotation=45)\n",
        "\n",
        "plt.suptitle(\"Radardiagram av klynger (uten standardisering)\", y=1.05, fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XuXDlGCG4i1C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neste steg.\n",
        "- FEATURE SCALING!\n",
        "- Snitt, standardavvik og utvikling p√• m√•nedlig bruk?\n",
        "- Vurdere *outlier detection* og behandling\n",
        " - Sjekk ut f.eks. `Isolation Forest ` algoritmen\n",
        "\n",
        "- PCA for dimensionality reduction\n",
        " - Fjerne redundans og st√∏y *(noice)*\n",
        " - Forenkle visualisering\n",
        " - Forbedre compute\n",
        " - Bedre clustering\n",
        "\n",
        "- Finne mest solgt produkter innen hvert cluster?\n",
        "  - Kan vi lage en produktanbefaler basert p√• dette?"
      ],
      "metadata": {
        "id": "cjoOG_mkLjLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature scaling\n",
        "\n",
        "K-means og PCA er veldig f√∏lsomme for avtander, s√• vi b√∏r standardisere kontinuerlige features for √• gi lik vekt til alle kolonnene.\n",
        "\n",
        "Is_UK og Day_Of_Week kan vi la st√•, siden bin√¶r og kategorisk (kodet som heltall) og trenger derfor ikke n√∏dvendigvis skalering i denne sammenhengen.\n",
        "\n",
        "CustomerID er kun en identifikator, den tas ogs√• ut av modelleringen."
      ],
      "metadata": {
        "id": "Rcvuw8nA3uuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialisere StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 1) Kolonner som ikke skal skaleres\n",
        "columns_to_exclude = ['CustomerID', 'Is_UK', 'Day_Of_Week']\n",
        "\n",
        "# 2) Velg numeriske kolonner som skal skaleres (alle minus unntakene er numeriske)\n",
        "columns_to_scale = customer_data.columns.difference(columns_to_exclude)\n",
        "\n",
        "# 3) Lage egen scaled dataframe\n",
        "customer_data_scaled = customer_data.copy()\n",
        "\n",
        "# 4) Standardiser: mean = 0, std = 1\n",
        "customer_data_scaled[columns_to_scale] = scaler.fit_transform(customer_data_scaled[columns_to_scale])\n",
        "\n",
        "# Se f√∏rste radene\n",
        "customer_data_scaled"
      ],
      "metadata": {
        "id": "d4L0iJKJ32qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Snitt, standardavvik og utvikling p√• m√•nedlig bruk?"
      ],
      "metadata": {
        "id": "Nzy-s8T-JrOJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Outlier detection og behandling?\n",
        "Sjekk ut `Isolation Forest` algoritmen"
      ],
      "metadata": {
        "id": "wI8YV7x2J1DE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA for dimensionality reduction"
      ],
      "metadata": {
        "id": "yd9C24KxJ5Fg"
      }
    }
  ]
}